{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to start training your custom Tensorflow model in AWS SageMaker\n",
    "# Training a tensorflow 2.1 model using a custom container\n",
    "\n",
    "Some sections of this notebook has been inspired by the tutorial:\n",
    "**SML Keras Training with Amazon SageMaker**\n",
    "https://github.com/pranaychandekar/keras-sagemaker-train\n",
    "\n",
    "**Script mode training with custom conyainer from sagemaker-examples**\n",
    "https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/custom-training-containers/script-mode-container/notebook/script-mode-container.ipynb\n",
    "\n",
    "In this notebook we will describe the most relevant steps to start training a custom algorithm in AWS SageMaker, not using a custom container, showing how to deal with experiments and solving some of the problems when facing with custom models when using SageMaker script mode on \n",
    "\n",
    "**Problem description**\n",
    "\n",
    "Following steps will be explained:  \n",
    "1. Create an Experiment and Trial to keep track of our experiments   \n",
    "2. Load the training data to our training instance\n",
    "3. Create the scripts to train our custom model, a Transformer.\n",
    "4. Create an Estimator to train our model in a Tensorflow 2.1 container in script mode\n",
    "5. Create a metric definitions to keep track of them in SageMaker\n",
    "4. Download the trained model to make predictions\n",
    "5. Resume training using the latest checkpoint from a previous training \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the environment and load the libraries\n",
    "\n",
    "Let's start by setting up the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import pickle\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.session.Session object at 0x7f60bdbddda0>\n",
      "arn:aws:iam::223817798831:role/service-role/AmazonSageMaker-ExecutionRole-20200708T194212\n",
      "us-east-1\n"
     ]
    }
   ],
   "source": [
    "print(sagemaker_session)\n",
    "print(role)\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define variables with data location and output location in S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column_list_file = 'iris_train_column_list.txt'\n",
    "data_folder_name='data'\n",
    "train_filename = 'spa.txt'\n",
    "non_breaking_en = 'nonbreaking_prefix.en'\n",
    "non_breaking_es = 'nonbreaking_prefix.es'\n",
    "trainedmodel_path = 'trained_model'\n",
    "output_data_path = 'output_data'\n",
    "model_info_file = 'model_info.pth'\n",
    "input_vocab_file = 'in_vocab.pkl'\n",
    "output_vocab_file = 'out_vocab.pkl'\n",
    "\n",
    "train_file = os.path.abspath(os.path.join(data_folder_name, train_filename))\n",
    "non_breaking_en_file = os.path.abspath(os.path.join(data_folder_name, non_breaking_en))\n",
    "non_breaking_es_file = os.path.abspath(os.path.join(data_folder_name, non_breaking_es))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the working bucket name for this project or experiment and the three locations in S3 we will deal with:\n",
    "- Training data\n",
    "- Model and Output data\n",
    "- Checkpoint data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your bucket name\n",
    "bucket_name = 'edumunozsala-ml-sagemaker'\n",
    "project_name = 'transformer-nmt-custom'\n",
    "\n",
    "training_data_folder = r'{}/data'.format(project_name)\n",
    "output_folder = r'{}'.format(project_name)\n",
    "ckpt_folder = r'{}/ckpt'.format(project_name)\n",
    "\n",
    "training_data_uri = r's3://' + bucket_name + r'/' + training_data_folder\n",
    "output_data_uri = r's3://' + bucket_name + r'/' + output_folder\n",
    "ckpt_data_uri = r's3://' + bucket_name + r'/' + ckpt_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s3://edumunozsala-ml-sagemaker/transformer-nmt-custom/data',\n",
       " 's3://edumunozsala-ml-sagemaker/transformer-nmt-custom',\n",
       " 's3://edumunozsala-ml-sagemaker/transformer-nmt-custom/ckpt')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_uri,output_data_uri,ckpt_data_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is not yet in the S3 folder we upload it in the next code section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://edumunozsala-ml-sagemaker/transformer-nmt-custom/data/nonbreaking_prefix.es'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_session.upload_data(train_file,\n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=training_data_folder)\n",
    "\n",
    "sagemaker_session.upload_data(non_breaking_en_file,\n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=training_data_folder)\n",
    "\n",
    "sagemaker_session.upload_data(non_breaking_es_file,\n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=training_data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an experiment and trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker-experiments\n",
      "  Using cached sagemaker_experiments-0.1.24-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: boto3>=1.12.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from sagemaker-experiments) (1.16.9)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.9 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from boto3>=1.12.8->sagemaker-experiments) (1.19.9)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.9->boto3>=1.12.8->sagemaker-experiments) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.25.4; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.9->boto3>=1.12.8->sagemaker-experiments) (1.25.10)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.9->boto3>=1.12.8->sagemaker-experiments) (1.14.0)\n",
      "Installing collected packages: sagemaker-experiments\n",
      "Successfully installed sagemaker-experiments-0.1.24\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the library necessary to handle experiments\n",
    "!pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries to work with Experiments in SageMaker\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the experiment name\n",
    "experiment_name='tf-transformer-custom'\n",
    "# Set the trial name \n",
    "trial_name=\"{}-{}\".format(experiment_name,'single-gpu-custom')\n",
    "\n",
    "tags = [{'Key': 'my-experiments', 'Value': 'transformerEngSpa1Custom'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create or load the experiment and the trial: **Explain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded experiment  tf-transformer-custom\n",
      "Loaded trial  tf-transformer-custom-single-gpu-custom\n"
     ]
    }
   ],
   "source": [
    "# create the experiment if it doesn't exist\n",
    "try:\n",
    "    training_experiment = Experiment.load(experiment_name=experiment_name)\n",
    "    print('Loaded experiment ',experiment_name)\n",
    "except Exception as ex:\n",
    "    if \"ResourceNotFound\" in str(ex):\n",
    "        training_experiment = Experiment.create(experiment_name=experiment_name,\n",
    "                                      description = \"Experiment to track trainings on my tensorflow Transformer Eng-Spa\", \n",
    "                                      tags = tags)\n",
    "        print('Created experiment ',experiment_name)\n",
    "# create the trial if it doesn't exist\n",
    "try:\n",
    "    single_gpu_trial = Trial.load(trial_name=trial_name)\n",
    "    print('Loaded trial ',trial_name)\n",
    "except Exception as ex:\n",
    "    if \"ResourceNotFound\" in str(ex):\n",
    "        single_gpu_trial = Trial.create(experiment_name=experiment_name, \n",
    "                             trial_name= trial_name,\n",
    "                             tags = tags)\n",
    "        print('Created trial ',trial_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a configuration definition for our experiment and trial\n",
    "trial_comp_name = 'single-gpu-custom-job'\n",
    "# Set the configuration parameters for the experiment\n",
    "experiment_config = {'ExperimentName': training_experiment.experiment_name, \n",
    "                       'TrialName': single_gpu_trial.trial_name,\n",
    "                       'TrialComponentDisplayName': trial_comp_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check and show information about the experiment and trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment:  tf-transformer-custom\n",
      "Trial:  tf-transformer-custom-single-gpu-custom\n"
     ]
    }
   ],
   "source": [
    "#\"{}-{}\".format(trail_name, experiment_name)\n",
    "print('Experiment: ',training_experiment.experiment_name)\n",
    "# Show the trials in the experiment\n",
    "for trial in training_experiment.list_trials():\n",
    "    print('Trial: ',trial.trial_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct a script for training\n",
    "\n",
    "This tutorial's training script was adapted from TensorFlow's official [CNN MNIST example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py). We have modified it to handle the ``model_dir`` parameter passed in by SageMaker. This is an S3 path which can be used for data sharing during distributed training and checkpointing and/or model persistence. We have also added an argument-parsing function to handle processing training-related variables.\n",
    "\n",
    "At the end of the training job we have added a step to export the trained model to the path stored in the environment variable ``SM_MODEL_DIR``, which always points to ``/opt/ml/model``. This is critical because SageMaker uploads all the model artifacts in this folder to S3 at end of training.\n",
    "\n",
    "**output_data_dir**\n",
    "\n",
    "**checkpoint_iru**\n",
    "\n",
    "Here is the entire script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[37m#import sagemaker_containers\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmath\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgc\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# To install tensorflow_datasets\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minstall\u001b[39;49;00m(package):\r\n",
      "    subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39;49;00m\u001b[33m-q\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, package])\r\n",
      "\r\n",
      "\u001b[37m# Install the library tensorflow_datasets\u001b[39;49;00m\r\n",
      "install(\u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow_datasets\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m preprocess_text_nonbreaking, subword_tokenize\r\n",
      "\u001b[37m#from utils_train import loss_function, CustomSchedule\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Transformer\r\n",
      "\r\n",
      "INPUT_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33minput\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "TARGET_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mtarget\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "\u001b[37m#NUM_SAMPLES = 80000 #40000\u001b[39;49;00m\r\n",
      "\u001b[37m#MAX_VOCAB_SIZE = 2**14\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m#BATCH_SIZE = 64  # Batch size for training.\u001b[39;49;00m\r\n",
      "\u001b[37m#EPOCHS = 10  # Number of epochs to train for.\u001b[39;49;00m\r\n",
      "\u001b[37m#MAX_LENGTH = 15\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_train_data\u001b[39;49;00m(training_dir, nonbreaking_in, nonbreaking_out, train_file, nsamples):\r\n",
      "    \u001b[37m# Load the nonbreaking files\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(training_dir, nonbreaking_in), \r\n",
      "        mode = \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, encoding = \u001b[33m\"\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        non_breaking_prefix_en = f.read()\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(training_dir, nonbreaking_out), \r\n",
      "        mode = \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, encoding = \u001b[33m\"\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        non_breaking_prefix_es = f.read()\r\n",
      "\r\n",
      "    non_breaking_prefix_en = non_breaking_prefix_en.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    non_breaking_prefix_en = [\u001b[33m'\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + pref + \u001b[33m'\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mfor\u001b[39;49;00m pref \u001b[35min\u001b[39;49;00m non_breaking_prefix_en]\r\n",
      "    non_breaking_prefix_es = non_breaking_prefix_es.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    non_breaking_prefix_es = [\u001b[33m'\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + pref + \u001b[33m'\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mfor\u001b[39;49;00m pref \u001b[35min\u001b[39;49;00m non_breaking_prefix_es]\r\n",
      "    \u001b[37m# Load the training data\u001b[39;49;00m\r\n",
      "    \u001b[37m# Load the dataset: sentence in english, sentence in spanish \u001b[39;49;00m\r\n",
      "    df=pd.read_csv(os.path.join(training_dir, train_file), sep=\u001b[33m\"\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, header=\u001b[34mNone\u001b[39;49;00m, names=[INPUT_COLUMN,TARGET_COLUMN], usecols=[\u001b[34m0\u001b[39;49;00m,\u001b[34m1\u001b[39;49;00m], \r\n",
      "               nrows=nsamples)\r\n",
      "    \u001b[37m# Preprocess the input data\u001b[39;49;00m\r\n",
      "    input_data=df[INPUT_COLUMN].apply(\u001b[34mlambda\u001b[39;49;00m x : preprocess_text_nonbreaking(x, non_breaking_prefix_en)).tolist()\r\n",
      "    \u001b[37m# Preprocess and include the end of sentence token to the target text\u001b[39;49;00m\r\n",
      "    target_data=df[TARGET_COLUMN].apply(\u001b[34mlambda\u001b[39;49;00m x : preprocess_text_nonbreaking(x, non_breaking_prefix_es)).tolist()\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m input_data, target_data\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain_train\u001b[39;49;00m(dataset, transformer, n_epochs, print_every=\u001b[34m50\u001b[39;49;00m):\r\n",
      "  \u001b[33m''' Train the transformer model for n_epochs using the data generator dataset'''\u001b[39;49;00m\r\n",
      "  losses = []\r\n",
      "  accuracies = []\r\n",
      "  \u001b[37m# In every epoch\u001b[39;49;00m\r\n",
      "  \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(n_epochs):\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mStarting epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(epoch+\u001b[34m1\u001b[39;49;00m))\r\n",
      "    start = time.time()\r\n",
      "    \u001b[37m# Reset the losss and accuracy calculations\u001b[39;49;00m\r\n",
      "    train_loss.reset_states()\r\n",
      "    train_accuracy.reset_states()\r\n",
      "    \u001b[37m# Get a batch of inputs and targets\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m (batch, (enc_inputs, targets)) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(dataset):\r\n",
      "        \u001b[37m# Set the decoder inputs\u001b[39;49;00m\r\n",
      "        dec_inputs = targets[:, :-\u001b[34m1\u001b[39;49;00m]\r\n",
      "        \u001b[37m# Set the target outputs, right shifted\u001b[39;49;00m\r\n",
      "        dec_outputs_real = targets[:, \u001b[34m1\u001b[39;49;00m:]\r\n",
      "        \u001b[34mwith\u001b[39;49;00m tf.GradientTape() \u001b[34mas\u001b[39;49;00m tape:\r\n",
      "            \u001b[37m# Call the transformer and get the predicted output\u001b[39;49;00m\r\n",
      "            predictions = transformer(enc_inputs, dec_inputs, \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "            \u001b[37m# Calculate the loss\u001b[39;49;00m\r\n",
      "            loss = loss_function(dec_outputs_real, predictions)\r\n",
      "        \u001b[37m# Update the weights and optimizer\u001b[39;49;00m\r\n",
      "        gradients = tape.gradient(loss, transformer.trainable_variables)\r\n",
      "        optimizer.apply_gradients(\u001b[36mzip\u001b[39;49;00m(gradients, transformer.trainable_variables))\r\n",
      "        \u001b[37m# Save and store the metrics\u001b[39;49;00m\r\n",
      "        train_loss(loss)\r\n",
      "        train_accuracy(dec_outputs_real, predictions)\r\n",
      "        \r\n",
      "        \u001b[34mif\u001b[39;49;00m batch % print_every == \u001b[34m0\u001b[39;49;00m:\r\n",
      "            losses.append(train_loss.result())\r\n",
      "            accuracies.append(train_accuracy.result())\r\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEpoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m Batch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m Loss \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m Accuracy \u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\r\n",
      "                epoch+\u001b[34m1\u001b[39;49;00m, batch, train_loss.result(), train_accuracy.result()))\r\n",
      "            \r\n",
      "    \u001b[37m# Checkpoint the model on every epoch        \u001b[39;49;00m\r\n",
      "    ckpt_save_path = ckpt_manager.save()\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving checkpoint for epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m in \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(epoch+\u001b[34m1\u001b[39;49;00m,\r\n",
      "                                                        ckpt_save_path))\r\n",
      "    \u001b[37m#print(\"Time for 1 epoch: {} secs\\n\".format(time.time() - start))\u001b[39;49;00m\r\n",
      "    \u001b[37m# Save the model\u001b[39;49;00m\r\n",
      "    \u001b[37m#transformer.save(args.sm_model_dir, overwrite=True, save_format='tf')\u001b[39;49;00m\r\n",
      "    \r\n",
      "  \u001b[34mreturn\u001b[39;49;00m losses, accuracies\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mloss_function\u001b[39;49;00m(target, pred):\r\n",
      "    mask = tf.math.logical_not(tf.math.equal(target, \u001b[34m0\u001b[39;49;00m))\r\n",
      "    loss_ = loss_object(target, pred)\r\n",
      "    \r\n",
      "    mask = tf.cast(mask, dtype=loss_.dtype)\r\n",
      "    loss_ *= mask\r\n",
      "    \r\n",
      "    \u001b[34mreturn\u001b[39;49;00m tf.reduce_mean(loss_)\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mCustomSchedule\u001b[39;49;00m(tf.keras.optimizers.schedules.LearningRateSchedule):\r\n",
      "    \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, d_model, warmup_steps=\u001b[34m4000\u001b[39;49;00m):\r\n",
      "        \u001b[36msuper\u001b[39;49;00m(CustomSchedule, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
      "        \r\n",
      "        \u001b[36mself\u001b[39;49;00m.d_model = tf.cast(d_model, tf.float32)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.warmup_steps = warmup_steps\r\n",
      "    \r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__call__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, step):\r\n",
      "        arg1 = tf.math.rsqrt(step)\r\n",
      "        arg2 = step * (\u001b[36mself\u001b[39;49;00m.warmup_steps**-\u001b[34m1.5\u001b[39;49;00m)\r\n",
      "        \r\n",
      "        \u001b[34mreturn\u001b[39;49;00m tf.math.rsqrt(\u001b[36mself\u001b[39;49;00m.d_model) * tf.math.minimum(arg1, arg2)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    \u001b[37m# Install tensorflow_datasets\u001b[39;49;00m\r\n",
      "    \u001b[37m#install('tensorflow_datasets')\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# All of the model parameters and training parameters are sent as arguments when the script\u001b[39;49;00m\r\n",
      "    \u001b[37m# is executed. Here we set up an argument parser to easily access the parameters.\u001b[39;49;00m\r\n",
      "\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m# Training Parameters\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max-len\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m15\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minput max sequence length for training (default: 60)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 2)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--nsamples\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10000\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of samples to train (default: 20000)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--resume\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mResume training from the latest checkpoint (default: False)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Data parameters                    \u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_file\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mTraining data file name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--non_breaking_in\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mNon breaking prefixes for input vocabulary\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--non_breaking_out\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mNon breaking prefixes for output vocabulary\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Model Parameters\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--d_model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mModel dimension (default: 64)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--ffn_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m128\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33msize of the FFN layer (default: 128)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--vocab_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10000\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33msize of the vocabulary (default: 10000)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--n_layers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of layers (default: 4)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--n_heads\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m8\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of heads (default: 8)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dropout_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.1\u001b[39;49;00m, metavar=\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mDropout rate (default: 0.1)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# SageMaker Parameters\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sm-model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    args = parser.parse_args()\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(args.sm_model_dir, args.model_dir)\r\n",
      "    \u001b[37m# Load the training data.\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mGet the train data\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    input_data, target_data = get_train_data(args.data_dir, args.non_breaking_in, args.non_breaking_out, args.train_file, args.nsamples)\r\n",
      "\r\n",
      "    \u001b[37m# Tokenize and pad the input sequences\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTokenize the input and output data and create the vocabularies\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \r\n",
      "    encoder_inputs, tokenizer_inputs, num_words_inputs, sos_token_input, eos_token_input, del_idx_inputs= subword_tokenize(input_data, \r\n",
      "                                                                                                        args.vocab_size, args.max_len)\r\n",
      "    \u001b[37m# Tokenize and pad the outputs sequences\u001b[39;49;00m\r\n",
      "    decoder_outputs, tokenizer_outputs, num_words_output, sos_token_output, eos_token_output, del_idx_outputs = subword_tokenize(target_data,                                                                                                       args.vocab_size, args.max_len)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mInput vocab: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,num_words_inputs)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mOutput vocab: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,num_words_output)\r\n",
      "    \r\n",
      "    \u001b[37m# Define a dataset \u001b[39;49;00m\r\n",
      "    dataset = tf.data.Dataset.from_tensor_slices(\r\n",
      "                    (encoder_inputs, decoder_outputs))\r\n",
      "    dataset = dataset.shuffle(\u001b[36mlen\u001b[39;49;00m(input_data), reshuffle_each_iteration=\u001b[34mTrue\u001b[39;49;00m).batch(\r\n",
      "                    args.batch_size, drop_remainder=\u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
      "\r\n",
      "    \u001b[37m# Clean the session\u001b[39;49;00m\r\n",
      "    tf.keras.backend.clear_session()\r\n",
      "    \u001b[37m# Create the Transformer model\u001b[39;49;00m\r\n",
      "    transformer = Transformer(vocab_size_enc=num_words_inputs,\r\n",
      "                          vocab_size_dec=num_words_output,\r\n",
      "                          d_model=args.d_model,\r\n",
      "                          n_layers=args.n_layers,\r\n",
      "                          FFN_units=args.ffn_dim,\r\n",
      "                          n_heads=args.n_heads,\r\n",
      "                          dropout_rate=args.dropout_rate)\r\n",
      "\r\n",
      "    \u001b[37m# Define a categorical cross entropy loss\u001b[39;49;00m\r\n",
      "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                                            reduction=\u001b[33m\"\u001b[39;49;00m\u001b[33mnone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Define a metric to store the mean loss of every epoch\u001b[39;49;00m\r\n",
      "    train_loss = tf.keras.metrics.Mean(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_loss\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Define a matric to save the accuracy in every epoch\u001b[39;49;00m\r\n",
      "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain_accuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Create the scheduler for learning rate decay\u001b[39;49;00m\r\n",
      "    leaning_rate = CustomSchedule(args.d_model)\r\n",
      "    \u001b[37m# Create the Adam optimizer\u001b[39;49;00m\r\n",
      "    optimizer = tf.keras.optimizers.Adam(leaning_rate,\r\n",
      "                                     beta_1=\u001b[34m0.9\u001b[39;49;00m,\r\n",
      "                                     beta_2=\u001b[34m0.98\u001b[39;49;00m,\r\n",
      "                                     epsilon=\u001b[34m1e-9\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m#Create the Checkpoint \u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCreating the checkpoint ...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    ckpt = tf.train.Checkpoint(transformer=transformer,\r\n",
      "                           optimizer=optimizer)\r\n",
      "\r\n",
      "    ckpt_manager = tf.train.CheckpointManager(ckpt, \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/checkpoints/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, max_to_keep=\u001b[34m1\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Restore from the latest checkpoint if requiered\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m ckpt_manager.latest_checkpoint \u001b[35mand\u001b[39;49;00m args.resume:\r\n",
      "        ckpt.restore(ckpt_manager.latest_checkpoint)\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLast checkpoint restored.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[37m# to save the model in tf 2.1.0\u001b[39;49;00m\r\n",
      "    \u001b[37m#print('Preparing the model to be saved....')\u001b[39;49;00m\r\n",
      "    \u001b[37m#for enc_inputs, targets in dataset.take(1):\u001b[39;49;00m\r\n",
      "    \u001b[37m#    dec_inputs = targets[:, :-1]\u001b[39;49;00m\r\n",
      "    \u001b[37m#    print (enc_inputs.shape, dec_inputs.shape)\u001b[39;49;00m\r\n",
      "    \u001b[37m#    transformer._set_inputs(enc_inputs, dec_inputs, True)\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Train the model\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTraining the model ....\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    losses, accuracies = main_train(dataset, transformer, args.epochs, \u001b[34m100\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Save the while model\u001b[39;49;00m\r\n",
      "    \u001b[37m# Save the entire model to a HDF5 file\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving the model ....\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    transformer.save_weights(os.path.join(args.sm_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), overwrite=\u001b[34mTrue\u001b[39;49;00m, save_format=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[37m#transformer.save_weights(args.sm_model_dir, overwrite=True, save_format='tf')\u001b[39;49;00m\r\n",
      "    \u001b[37m# Save the parameters used to construct the model\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving the model parameters\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    model_info_path = os.path.join(args.output_data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        model_info = {\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size_enc\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: num_words_inputs,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size_dec\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: num_words_output,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33msos_token_input\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: sos_token_input,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33meos_token_input\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: eos_token_input,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33msos_token_output\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: sos_token_output,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33meos_token_output\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: eos_token_output,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mn_layers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.n_layers,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33md_model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.d_model,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mffn_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.ffn_dim,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mn_heads\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.n_heads,\r\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mdrop_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.dropout_rate\r\n",
      "        }\r\n",
      "        pickle.dump(model_info, f)\r\n",
      "          \r\n",
      "\t\u001b[37m# Save the tokenizers with the vocabularies\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving the dictionaries ....\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    vocabulary_in = os.path.join(args.output_data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33min_vocab.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(vocabulary_in, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        pickle.dump(tokenizer_inputs, f)\r\n",
      "\r\n",
      "    vocabulary_out = os.path.join(args.output_data_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mout_vocab.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(vocabulary_out, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "        pickle.dump(tokenizer_outputs, f)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 'train/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the custom container image and register in Amazon ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "a='Dockerfile.gpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{a}\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "echo {a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Stopping docker: [  OK  ]\r\n",
      "Starting docker:\t.[  OK  ]\r\n",
      "Sending build context to Docker daemon  42.53MB\r",
      "\r\n",
      "Step 1/14 : FROM tensorflow/tensorflow:latest-gpu-py3\n",
      "latest-gpu-py3: Pulling from tensorflow/tensorflow\n",
      "7ddbc47eeb70: Pulling fs layer\n",
      "c1bbdc448b72: Pulling fs layer\n",
      "8c3b70e39044: Pulling fs layer\n",
      "45d437916d57: Pulling fs layer\n",
      "d8f1569ddae6: Pulling fs layer\n",
      "85386706b020: Pulling fs layer\n",
      "ee9b457b77d0: Pulling fs layer\n",
      "bebfcc1316f7: Pulling fs layer\n",
      "644140fd95a9: Pulling fs layer\n",
      "d6c0f989e873: Pulling fs layer\n",
      "7a8e64f26211: Pulling fs layer\n",
      "c33b03e4dd22: Pulling fs layer\n",
      "bca93af797c1: Pulling fs layer\n",
      "47f6c197be35: Pulling fs layer\n",
      "e5da48aa9554: Pulling fs layer\n",
      "ca68d98a90c4: Pulling fs layer\n",
      "644140fd95a9: Waiting\n",
      "d6c0f989e873: Waiting\n",
      "7a8e64f26211: Waiting\n",
      "45d437916d57: Waiting\n",
      "c33b03e4dd22: Waiting\n",
      "bca93af797c1: Waiting\n",
      "d8f1569ddae6: Waiting\n",
      "85386706b020: Waiting\n",
      "ee9b457b77d0: Waiting\n",
      "bebfcc1316f7: Waiting\n",
      "ca68d98a90c4: Waiting\n",
      "47f6c197be35: Waiting\n",
      "e5da48aa9554: Waiting\n",
      "8c3b70e39044: Verifying Checksum\n",
      "8c3b70e39044: Download complete\n",
      "c1bbdc448b72: Download complete\n",
      "45d437916d57: Verifying Checksum\n",
      "45d437916d57: Download complete\n",
      "d8f1569ddae6: Verifying Checksum\n",
      "d8f1569ddae6: Download complete\n",
      "85386706b020: Verifying Checksum\n",
      "85386706b020: Download complete\n",
      "7ddbc47eeb70: Verifying Checksum\n",
      "7ddbc47eeb70: Download complete\n",
      "ee9b457b77d0: Verifying Checksum\n",
      "ee9b457b77d0: Download complete\n",
      "d6c0f989e873: Download complete\n",
      "7a8e64f26211: Verifying Checksum\n",
      "7a8e64f26211: Download complete\n",
      "c33b03e4dd22: Verifying Checksum\n",
      "c33b03e4dd22: Download complete\n",
      "7ddbc47eeb70: Pull complete\n",
      "bca93af797c1: Verifying Checksum\n",
      "bca93af797c1: Download complete\n",
      "c1bbdc448b72: Pull complete\n",
      "8c3b70e39044: Pull complete\n",
      "45d437916d57: Pull complete\n",
      "644140fd95a9: Verifying Checksum\n",
      "644140fd95a9: Download complete\n",
      "e5da48aa9554: Verifying Checksum\n",
      "e5da48aa9554: Download complete\n",
      "ca68d98a90c4: Verifying Checksum\n",
      "ca68d98a90c4: Download complete\n",
      "d8f1569ddae6: Pull complete\n",
      "85386706b020: Pull complete\n",
      "ee9b457b77d0: Pull complete\n",
      "bebfcc1316f7: Verifying Checksum\n",
      "bebfcc1316f7: Download complete\n",
      "47f6c197be35: Verifying Checksum\n",
      "47f6c197be35: Download complete\n",
      "bebfcc1316f7: Pull complete\n",
      "644140fd95a9: Pull complete\n",
      "d6c0f989e873: Pull complete\n",
      "7a8e64f26211: Pull complete\n",
      "c33b03e4dd22: Pull complete\n",
      "bca93af797c1: Pull complete\n",
      "47f6c197be35: Pull complete\n",
      "e5da48aa9554: Pull complete\n",
      "ca68d98a90c4: Pull complete\n",
      "Digest: sha256:1010e051dde4a9b62532a80f4a9a619013eafc78491542d5ef5da796cc2697ae\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:latest-gpu-py3\n",
      " ---> e2a4af785bdb\n",
      "Step 2/14 : MAINTAINER edumunozsala \"edumunozsala@gmail.com\"\n",
      " ---> Running in 56a6a6fd7592\n",
      "Removing intermediate container 56a6a6fd7592\n",
      " ---> 7340925587ee\n",
      "Step 3/14 : LABEL project=\"transformer-mnt-custom\"\n",
      " ---> Running in 1070e381b786\n",
      "Removing intermediate container 1070e381b786\n",
      " ---> 155cdc11c57e\n",
      "Step 4/14 : ARG APP_HOME=/opt/program\n",
      " ---> Running in e724e66d07ce\n",
      "Removing intermediate container e724e66d07ce\n",
      " ---> 332007acfc08\n",
      "Step 5/14 : ARG SOURCE_CODE=/opt/ml/code/\n",
      " ---> Running in 586996f8edc9\n",
      "Removing intermediate container 586996f8edc9\n",
      " ---> f7b1cefa6935\n",
      "Step 6/14 : ARG PIP=pip3\n",
      " ---> Running in e04b444a68d9\n",
      "Removing intermediate container e04b444a68d9\n",
      " ---> 878876f8bbb8\n",
      "Step 7/14 : ENV PATH=\"${APP_HOME}:${PATH}\"\n",
      " ---> Running in b784e3f34b6c\n",
      "Removing intermediate container b784e3f34b6c\n",
      " ---> 69677d00aebb\n",
      "Step 8/14 : RUN ${PIP} install --no-cache --upgrade         pip         setuptools\n",
      " ---> Running in 3565e4b2502e\n",
      "Collecting pip\n",
      "  Downloading https://files.pythonhosted.org/packages/cb/28/91f26bd088ce8e22169032100d4260614fc3da435025ff389ef1d396a433/pip-20.2.4-py2.py3-none-any.whl (1.5MB)\n",
      "Collecting setuptools\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/38/c21ef5034684ffc0412deefbb07d66678332290c14bb5269c85145fbd55e/setuptools-50.3.2-py3-none-any.whl (785kB)\n",
      "Installing collected packages: pip, setuptools\n",
      "  Found existing installation: pip 19.3.1\n",
      "    Uninstalling pip-19.3.1:\n",
      "      Successfully uninstalled pip-19.3.1\n",
      "  Found existing installation: setuptools 44.0.0\n",
      "    Uninstalling setuptools-44.0.0:\n",
      "      Successfully uninstalled setuptools-44.0.0\n",
      "Successfully installed pip-20.2.4 setuptools-50.3.2\n",
      "Removing intermediate container 3565e4b2502e\n",
      " ---> 33b239ae1d6b\n",
      "Step 9/14 : RUN ${PIP} install --no-cache --upgrade     sagemaker-training\n",
      " ---> Running in d417c27b66cc\n",
      "Collecting sagemaker-training\n",
      "  Downloading sagemaker_training-3.6.3.tar.gz (40 kB)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sagemaker-training) (1.18.1)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.16.14-py2.py3-none-any.whl (129 kB)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from sagemaker-training) (1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: pip in /usr/local/lib/python3.6/dist-packages (from sagemaker-training) (20.2.4)\n",
      "Collecting retrying>=1.3.3\n",
      "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
      "Collecting gevent\n",
      "  Downloading gevent-20.9.0-cp36-cp36m-manylinux2010_x86_64.whl (5.3 MB)\n",
      "Collecting inotify_simple==1.2.1\n",
      "  Downloading inotify_simple-1.2.1.tar.gz (7.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.15.5 in /usr/local/lib/python3.6/dist-packages (from sagemaker-training) (0.16.0)\n",
      "Collecting paramiko>=2.4.2\n",
      "  Downloading paramiko-2.7.2-py2.py3-none-any.whl (206 kB)\n",
      "Collecting psutil>=5.6.7\n",
      "  Downloading psutil-5.7.3.tar.gz (465 kB)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.1 in /usr/local/lib/python3.6/dist-packages (from sagemaker-training) (3.11.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.2.2 in /usr/local/lib/python3.6/dist-packages (from sagemaker-training) (1.4.1)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.20.0,>=1.19.14\n",
      "  Downloading botocore-1.19.14-py2.py3-none-any.whl (6.7 MB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting zope.interface\n",
      "  Downloading zope.interface-5.2.0-cp36-cp36m-manylinux2010_x86_64.whl (236 kB)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from gevent->sagemaker-training) (50.3.2)\n",
      "Collecting greenlet>=0.4.17; platform_python_implementation == \"CPython\"\n",
      "  Downloading greenlet-0.4.17-cp36-cp36m-manylinux1_x86_64.whl (44 kB)\n",
      "Collecting zope.event\n",
      "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
      "Collecting bcrypt>=3.1.3\n",
      "  Downloading bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63 kB)\n",
      "Collecting pynacl>=1.0.1\n",
      "  Downloading PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961 kB)\n",
      "Collecting cryptography>=2.5\n",
      "  Downloading cryptography-3.2.1-cp35-abi3-manylinux2010_x86_64.whl (2.6 MB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1\n",
      "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.26,>=1.25.4; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.14->boto3->sagemaker-training) (1.25.7)\n",
      "Collecting cffi>=1.1\n",
      "  Downloading cffi-1.14.3-cp36-cp36m-manylinux1_x86_64.whl (400 kB)\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "Building wheels for collected packages: sagemaker-training, retrying, inotify-simple, psutil\n",
      "  Building wheel for sagemaker-training (setup.py): started\n",
      "  Building wheel for sagemaker-training (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker-training: filename=sagemaker_training-3.6.3-cp36-cp36m-linux_x86_64.whl size=73215 sha256=bf0392c8005c3e0a21e9fa6d2629a89ac8a8cf663d730a568b1eb9c8fcd6d4f9\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ube0m457/wheels/3c/ed/2b/50bb000c63ebe38b08be901b9dd1f0caf35cb59973d070dbab\n",
      "  Building wheel for retrying (setup.py): started\n",
      "  Building wheel for retrying (setup.py): finished with status 'done'\n",
      "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=9530 sha256=6d373de446267118358390355b6a02b7b40eb12affef6c00b05c3f676c2a1a90\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ube0m457/wheels/ac/cb/8a/b27bf6323e2f4c462dcbf77d70b7c5e7868a7fbe12871770cf\n",
      "  Building wheel for inotify-simple (setup.py): started\n",
      "  Building wheel for inotify-simple (setup.py): finished with status 'done'\n",
      "  Created wheel for inotify-simple: filename=inotify_simple-1.2.1-py3-none-any.whl size=10083 sha256=d2fefa6c22e04a4ac7e4f865c1be46f7edc7ad8a0507bf1f7874b6e1ef04f5c7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ube0m457/wheels/a5/34/f6/5d4eae9a326dc637344cd68353ddd119e9f6f575e925502888\n",
      "  Building wheel for psutil (setup.py): started\n",
      "  Building wheel for psutil (setup.py): finished with status 'done'\n",
      "  Created wheel for psutil: filename=psutil-5.7.3-cp36-cp36m-linux_x86_64.whl size=288522 sha256=fcebb31bca4798cd18d1d2abe3e5407deb756ccda2e8e5ec561239561c0c9d0e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ube0m457/wheels/fa/ad/67/90bbaacdcfe970960dd5158397f23a6579b51d853720d7856d\n",
      "Successfully built sagemaker-training retrying inotify-simple psutil\n",
      "Installing collected packages: jmespath, python-dateutil, botocore, s3transfer, boto3, retrying, zope.interface, greenlet, zope.event, gevent, inotify-simple, pycparser, cffi, bcrypt, pynacl, cryptography, paramiko, psutil, sagemaker-training\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 2.1.4\n",
      "    Uninstalling cryptography-2.1.4:\n",
      "      Successfully uninstalled cryptography-2.1.4\n",
      "Successfully installed bcrypt-3.2.0 boto3-1.16.14 botocore-1.19.14 cffi-1.14.3 cryptography-3.2.1 gevent-20.9.0 greenlet-0.4.17 inotify-simple-1.2.1 jmespath-0.10.0 paramiko-2.7.2 psutil-5.7.3 pycparser-2.20 pynacl-1.4.0 python-dateutil-2.8.1 retrying-1.3.3 s3transfer-0.3.3 sagemaker-training-3.6.3 zope.event-4.5.0 zope.interface-5.2.0\n",
      "Removing intermediate container d417c27b66cc\n",
      " ---> 898735ac8372\n",
      "Step 10/14 : ADD train/requirements.txt /\n",
      " ---> f40b206ea429\n",
      "Step 11/14 : RUN ${PIP} install -r requirements.txt\n",
      " ---> Running in 050a74a449b7\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.4-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.18.1)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.3.1-cp36-cp36m-manylinux2010_x86_64.whl (320.4 MB)\n",
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.1.0-py3-none-any.whl (3.6 MB)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.4-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.1.0)\n",
      "Collecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.3.0-py3-none-any.whl (6.8 MB)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.11.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.1.8)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.10.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.13.0)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.26.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.11.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.30.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.25.0-py3-none-any.whl (44 kB)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n",
      "Collecting typing-extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.7-py3-none-any.whl (18 kB)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "Collecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-3.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 4)) (2.22.0)\n",
      "Collecting attrs>=18.1.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (0.16.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (50.3.2)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (1.10.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (3.1.1)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "Collecting zipp>=0.4; python_version < \"3.8\"\n",
      "  Downloading zipp-3.4.0-py3-none-any.whl (5.2 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets->-r requirements.txt (line 4)) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->tensorflow_datasets->-r requirements.txt (line 4)) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets->-r requirements.txt (line 4)) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets->-r requirements.txt (line 4)) (1.25.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (4.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (0.2.8)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (4.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (3.1.0)\n",
      "Building wheels for collected packages: promise, future\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=23950 sha256=38014c4a7dcb6c91b07bd7d6568379349ae47e30339b362ee760e4c2cb9e3983\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=493275 sha256=90c635d443702c100488cfd70ef47402907ca735f4c55caa488bcd9075ba97b1\n",
      "  Stored in directory: /root/.cache/pip/wheels/6e/9c/ed/4499c9865ac1002697793e0ae05ba6be33553d098f3347fb94\n",
      "Successfully built promise future\n",
      "Installing collected packages: pytz, pandas, tensorboard-plugin-wit, tensorboard, gast, tensorflow-estimator, keras-preprocessing, astunparse, tensorflow, googleapis-common-protos, tensorflow-metadata, promise, tqdm, typing-extensions, dataclasses, future, zipp, importlib-resources, attrs, dill, tensorflow-datasets\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.1.0\n",
      "    Uninstalling tensorboard-2.1.0:\n",
      "      Successfully uninstalled tensorboard-2.1.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.2.2\n",
      "    Uninstalling gast-0.2.2:\n",
      "      Successfully uninstalled gast-0.2.2\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.1.0\n",
      "    Uninstalling tensorflow-estimator-2.1.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.1.0\n",
      "  Attempting uninstall: keras-preprocessing\n",
      "    Found existing installation: Keras-Preprocessing 1.1.0\n",
      "    Uninstalling Keras-Preprocessing-1.1.0:\n",
      "      Successfully uninstalled Keras-Preprocessing-1.1.0\n",
      "\u001b[91mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorflow-gpu 2.1.0 requires gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\n",
      "tensorflow-gpu 2.1.0 requires tensorboard<2.2.0,>=2.1.0, but you'll have tensorboard 2.3.0 which is incompatible.\n",
      "tensorflow-gpu 2.1.0 requires tensorflow-estimator<2.2.0,>=2.1.0rc0, but you'll have tensorflow-estimator 2.3.0 which is incompatible.\n",
      "\u001b[0mSuccessfully installed astunparse-1.6.3 attrs-20.3.0 dataclasses-0.7 dill-0.3.3 future-0.18.2 gast-0.3.3 googleapis-common-protos-1.52.0 importlib-resources-3.3.0 keras-preprocessing-1.1.2 pandas-1.1.4 promise-2.3 pytz-2020.4 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-datasets-4.1.0 tensorflow-estimator-2.3.0 tensorflow-metadata-0.25.0 tqdm-4.51.0 typing-extensions-3.7.4.3 zipp-3.4.0\n",
      "Removing intermediate container 050a74a449b7\n",
      " ---> 01f1b84e4d9e\n",
      "Step 12/14 : COPY train/* ${SOURCE_CODE}\n",
      " ---> 49b7733bb214\n",
      "Step 13/14 : WORKDIR ${SOURCE_CODE}\n",
      " ---> Running in 22c5d8f70f4e\n",
      "Removing intermediate container 22c5d8f70f4e\n",
      " ---> b0673a882c88\n",
      "Step 14/14 : ENV SAGEMAKER_PROGRAM train.py\n",
      " ---> Running in ffdf92817e63\n",
      "Removing intermediate container ffdf92817e63\n",
      " ---> 5aca28164885\n",
      "Successfully built 5aca28164885\n",
      "Successfully tagged transformer-nmt-custom-gpu:latest\n",
      "The push refers to repository [223817798831.dkr.ecr.us-east-1.amazonaws.com/transformer-nmt-custom-gpu]\n",
      "438c7719770d: Preparing\n",
      "0e71f4d9bd27: Preparing\n",
      "9a61b94f3840: Preparing\n",
      "af7b03704dbc: Preparing\n",
      "9733c03c871e: Preparing\n",
      "8bdbbc38d97a: Preparing\n",
      "6e576898edb7: Preparing\n",
      "b8f75ed65728: Preparing\n",
      "2acb7acd7288: Preparing\n",
      "9e34e92e6b95: Preparing\n",
      "e24139c523b9: Preparing\n",
      "51997f5d204b: Preparing\n",
      "5c4b8cd3b09b: Preparing\n",
      "e44ff087319e: Preparing\n",
      "808fd332a58a: Preparing\n",
      "b16af11cbf29: Preparing\n",
      "37b9a4b22186: Preparing\n",
      "e0b3afb09dc3: Preparing\n",
      "6c01b5a53aac: Preparing\n",
      "2c6ac8e5063e: Preparing\n",
      "cc967c529ced: Preparing\n",
      "8bdbbc38d97a: Waiting\n",
      "6e576898edb7: Waiting\n",
      "b8f75ed65728: Waiting\n",
      "2acb7acd7288: Waiting\n",
      "9e34e92e6b95: Waiting\n",
      "e24139c523b9: Waiting\n",
      "51997f5d204b: Waiting\n",
      "5c4b8cd3b09b: Waiting\n",
      "e44ff087319e: Waiting\n",
      "808fd332a58a: Waiting\n",
      "b16af11cbf29: Waiting\n",
      "37b9a4b22186: Waiting\n",
      "e0b3afb09dc3: Waiting\n",
      "6c01b5a53aac: Waiting\n",
      "2c6ac8e5063e: Waiting\n",
      "cc967c529ced: Waiting\n",
      "438c7719770d: Pushed\n",
      "9a61b94f3840: Pushed\n",
      "8bdbbc38d97a: Pushed\n",
      "6e576898edb7: Pushed\n",
      "9733c03c871e: Pushed\n",
      "af7b03704dbc: Pushed\n",
      "2acb7acd7288: Pushed\n",
      "51997f5d204b: Pushed\n",
      "9e34e92e6b95: Pushed\n",
      "e24139c523b9: Pushed\n",
      "5c4b8cd3b09b: Pushed\n",
      "808fd332a58a: Pushed\n",
      "b16af11cbf29: Pushed\n",
      "e0b3afb09dc3: Pushed\n",
      "37b9a4b22186: Pushed\n",
      "6c01b5a53aac: Pushed\n",
      "2c6ac8e5063e: Pushed\n",
      "cc967c529ced: Pushed\n",
      "e44ff087319e: Pushed\n",
      "0e71f4d9bd27: Pushed\n",
      "b8f75ed65728: Pushed\n",
      "latest: digest: sha256:60c34fcae1995f5b978b6a878868b6c1ba53f88ce896b9a0ec69f83f26725e2a size: 4727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=transformer-nmt-custom-gpu\n",
    "\n",
    "chmod +x train/*\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "# On a SageMaker Notebook Instance, the docker daemon may need to be restarted in order\n",
    "# to detect your network configuration correctly.  (This is a known issue.)\n",
    "if [ -d \"/home/ec2-user/SageMaker\" ]; then\n",
    "  sudo service docker restart\n",
    "fi\n",
    "\n",
    "# Comment the line below to use a GPU\n",
    "#docker build  -t ${algorithm_name} -f Dockerfile.cpu .\n",
    "\n",
    "# Uncomment the below line if you wish to run on a GPU\n",
    "docker build  -t ${algorithm_name} -f Dockerfile.gpu . \n",
    "\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a training job using the `TensorFlow` estimator\n",
    "\n",
    "The `sagemaker.tensorflow.TensorFlow` estimator handles locating the script mode container, uploading your script to a S3 location and creating a SageMaker training job. Let's call out a couple important parameters here:\n",
    "\n",
    "* `py_version` is set to `'py3'` to indicate that we are using script mode since legacy mode supports only Python 2. Though Python 2 will be deprecated soon, you can use script mode with Python 2 by setting `py_version` to `'py2'` and `script_mode` to `True`.\n",
    "\n",
    "* `distributions` is used to configure the distributed training setup. It's required only if you are doing distributed training either across a cluster of instances or across multiple GPUs. Here we are using parameter servers as the distributed training schema. SageMaker training jobs run on homogeneous clusters. To make parameter server more performant in the SageMaker setup, we run a parameter server on every instance in the cluster, so there is no need to specify the number of parameter servers to launch. Script mode also supports distributed training with [Horovod](https://github.com/horovod/horovod). You can find the full documentation on how to configure `distributions` [here](https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/tensorflow#distributed-training). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.estimator import Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables for account, region and container image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "container image uri:  223817798831.dkr.ecr.us-east-1.amazonaws.com/transformer-nmt-custom-gpu\n"
     ]
    }
   ],
   "source": [
    "account = boto3.client('sts').get_caller_identity().get('Account') # aws account \n",
    "#container_image = '{}.dkr.ecr.{}.amazonaws.com/{}'.format(account, region, project_name) # algorithm image path in ECR\n",
    "container_image = '{}.dkr.ecr.{}.amazonaws.com/{}-gpu'.format(account, region, project_name) # algorithm image path in ECR\n",
    "print('container image uri: ',container_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also initiate an estimator to train with TensorFlow 2.1 script. The only things that you will need to change are the script name and ``framewotk_version``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instance_type='ml.m5.xlarge'\n",
    "#instance_type='ml.m4.4xlarge'\n",
    "instance_type='ml.p2.xlarge'\n",
    "#instance_type='local'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define use of checkpoint and resumen\n",
    "How to use, how it works\n",
    "\n",
    "The local path that the algorithm writes its checkpoints to. SageMaker will persist all files under this path to checkpoint_s3_uri continually during training. On job startup the reverse happens - data from the s3 location is downloaded to this path before the algorithm is started. If the path is unset then SageMaker assumes the checkpoints will be provided under /opt/ml/checkpoints/.\n",
    "\n",
    "- Define the use of metrics\n",
    "You can monitor the metrics that a training job emits in real time in the **CloudWatch console**\n",
    "To monitor training job metrics (CloudWatch console)\n",
    "\n",
    "Open the CloudWatch console at https://console.aws.amazon.com/cloudwatch/.\n",
    "\n",
    "Choose Metrics, then choose /aws/sagemaker/TrainingJobs.\n",
    "\n",
    "Choose TrainingJobName.\n",
    "\n",
    "On the All metrics tab, choose the names of the training metrics that you want to monitor.\n",
    "\n",
    "On the Graphed metrics tab, configure the graph options. For more information about using CloudWatch graphs, see Graph Metrics in the Amazon CloudWatch User Guide\n",
    "\n",
    "You can monitor the metrics that a training job emits in real time by using the **SageMaker console**.\n",
    "\n",
    "To monitor training job metrics (SageMaker console)\n",
    "\n",
    "Open the SageMaker console at https://console.aws.amazon.com/sagemaker/.\n",
    "\n",
    "Choose Training jobs, then choose the training job whose metrics you want to see.\n",
    "\n",
    "Choose TrainingJobName.\n",
    "\n",
    "In the Monitor section, you can review the graphs of instance utilization and algorithm metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics to search for\n",
    "metric_definitions = [{'Name': 'loss', 'Regex': 'Loss ([0-9\\\\.]+)'},{'Name': 'Accuracy', 'Regex': 'Accuracy ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = Estimator(#entry_point='train.py',\n",
    "                       #source_dir=\"train\",\n",
    "                       role=role,\n",
    "                       instance_count=1,\n",
    "                       instance_type=instance_type,\n",
    "                       #framework_version='2.1.0',\n",
    "                       image_uri=container_image,\n",
    "                       #py_version='py3',\n",
    "                       output_path=output_data_uri,\n",
    "                       code_location=output_data_uri,\n",
    "                       base_job_name='tf-transformer',\n",
    "                       script_mode= True,\n",
    "                       #checkpoint_local_path = 'ckpt',\n",
    "                       checkpoint_s3_uri = ckpt_data_uri,\n",
    "                       metric_definitions = metric_definitions, \n",
    "                       hyperparameters={\n",
    "                        'epochs': 5,\n",
    "                        'nsamples': 20000,\n",
    "                        'resume': False,\n",
    "                        'train_file': 'spa.txt',\n",
    "                        'non_breaking_in': 'nonbreaking_prefix.en',\n",
    "                        'non_breaking_out': 'nonbreaking_prefix.es'\n",
    "                       })\n",
    "                       #distributions={'parameter_server': {'enabled': False}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling ``fit``\n",
    "\n",
    "To start a training job, we call `estimator.fit(training_data_uri)`.\n",
    "\n",
    "An S3 location is used here as the input. `fit` creates a default channel named `'training'`, which points to this S3 location. In the training script we can then access the training data from the location stored in `SM_CHANNEL_TRAINING`. `fit` accepts a couple other types of input as well. See the API doc [here](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.EstimatorBase.fit) for details.\n",
    "\n",
    "When training starts, the TensorFlow container executes mnist.py, passing `hyperparameters` and `model_dir` from the estimator as script arguments. Because we didn't define either in this example, no hyperparameters are passed, and `model_dir` defaults to `s3://<DEFAULT_BUCKET>/<TRAINING_JOB_NAME>`, so the script execution is as follows:\n",
    "```bash\n",
    "python mnist.py --model_dir s3://<DEFAULT_BUCKET>/<TRAINING_JOB_NAME>\n",
    "```\n",
    "When training is complete, the training job will upload the saved model for TensorFlow serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling fit to train a model with TensorFlow 2.1 scroipt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf-transformer-custom-single-gpu-custom-2020-11-10-17-42-47\n"
     ]
    }
   ],
   "source": [
    "#job_name=f'tensorflow-single-gpu-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())}'\n",
    "job_name = '{}-{}'.format(trial_name,time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime()))\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: tf-transformer-custom-single-gpu-custom-2020-11-10-17-42-47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-10 17:42:52 Starting - Starting the training job...\n",
      "2020-11-10 17:42:54 Starting - Launching requested ML instances.........\n",
      "2020-11-10 17:44:27 Starting - Preparing the instances for training......\n",
      "2020-11-10 17:45:40 Downloading - Downloading input data...\n",
      "2020-11-10 17:46:10 Training - Downloading the training image...............\n",
      "2020-11-10 17:48:46 Training - Training image download completed. Training in progress.\u001b[34m2020-11-10 17:48:46,233 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2020.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.11.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.30.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.1.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (3.11.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.26.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (2.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 3)) (1.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 4)) (3.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 4)) (0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 4)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 4)) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 4)) (4.51.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 4)) (0.25.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 4)) (2.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 4)) (0.18.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 4)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r requirements.txt (line 4)) (20.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow->-r requirements.txt (line 3)) (50.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (0.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (1.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (0.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (3.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (1.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets->-r requirements.txt (line 4)) (3.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets->-r requirements.txt (line 4)) (1.52.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets->-r requirements.txt (line 4)) (2019.11.28)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->tensorflow_datasets->-r requirements.txt (line 4)) (2.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets->-r requirements.txt (line 4)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets->-r requirements.txt (line 4)) (1.25.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (0.2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->-r requirements.txt (line 3)) (0.4.8)\u001b[0m\n",
      "\u001b[34m2020-11-10 17:48:47,885 sagemaker-training-toolkit INFO     Failed to parse hyperparameter resume value False to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-11-10 17:48:47,886 sagemaker-training-toolkit INFO     Failed to parse hyperparameter non_breaking_out value nonbreaking_prefix.es to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-11-10 17:48:47,886 sagemaker-training-toolkit INFO     Failed to parse hyperparameter train_file value spa.txt to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-11-10 17:48:47,886 sagemaker-training-toolkit INFO     Failed to parse hyperparameter non_breaking_in value nonbreaking_prefix.en to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-11-10 17:48:51,049 sagemaker-training-toolkit INFO     Failed to parse hyperparameter resume value False to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-11-10 17:48:51,050 sagemaker-training-toolkit INFO     Failed to parse hyperparameter non_breaking_out value nonbreaking_prefix.es to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-11-10 17:48:51,050 sagemaker-training-toolkit INFO     Failed to parse hyperparameter train_file value spa.txt to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-11-10 17:48:51,050 sagemaker-training-toolkit INFO     Failed to parse hyperparameter non_breaking_in value nonbreaking_prefix.en to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-11-10 17:48:51,074 sagemaker-training-toolkit INFO     Failed to parse hyperparameter resume value False to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-11-10 17:48:51,074 sagemaker-training-toolkit INFO     Failed to parse hyperparameter non_breaking_out value nonbreaking_prefix.es to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-11-10 17:48:51,074 sagemaker-training-toolkit INFO     Failed to parse hyperparameter train_file value spa.txt to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-11-10 17:48:51,074 sagemaker-training-toolkit INFO     Failed to parse hyperparameter non_breaking_in value nonbreaking_prefix.en to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2020-11-10 17:48:51,085 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": null,\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"resume\": \"False\",\n",
      "        \"non_breaking_out\": \"nonbreaking_prefix.es\",\n",
      "        \"nsamples\": 20000,\n",
      "        \"train_file\": \"spa.txt\",\n",
      "        \"non_breaking_in\": \"nonbreaking_prefix.en\",\n",
      "        \"epochs\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tf-transformer-custom-single-gpu-custom-2020-11-10-17-42-47\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/code\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":5,\"non_breaking_in\":\"nonbreaking_prefix.en\",\"non_breaking_out\":\"nonbreaking_prefix.es\",\"nsamples\":20000,\"resume\":\"False\",\"train_file\":\"spa.txt\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/code\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":null,\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":5,\"non_breaking_in\":\"nonbreaking_prefix.en\",\"non_breaking_out\":\"nonbreaking_prefix.es\",\"nsamples\":20000,\"resume\":\"False\",\"train_file\":\"spa.txt\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tf-transformer-custom-single-gpu-custom-2020-11-10-17-42-47\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"5\",\"--non_breaking_in\",\"nonbreaking_prefix.en\",\"--non_breaking_out\",\"nonbreaking_prefix.es\",\"--nsamples\",\"20000\",\"--resume\",\"False\",\"--train_file\",\"spa.txt\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_RESUME=False\u001b[0m\n",
      "\u001b[34mSM_HP_NON_BREAKING_OUT=nonbreaking_prefix.es\u001b[0m\n",
      "\u001b[34mSM_HP_NSAMPLES=20000\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=spa.txt\u001b[0m\n",
      "\u001b[34mSM_HP_NON_BREAKING_IN=nonbreaking_prefix.en\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 train.py --epochs 5 --non_breaking_in nonbreaking_prefix.en --non_breaking_out nonbreaking_prefix.es --nsamples 20000 --resume False --train_file spa.txt\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2020-11-10 17:48:52.684091: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mRequirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (4.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (4.51.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.18.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (3.7.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.25.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (3.11.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (3.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (20.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.18.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.25.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2019.11.28)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.52.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow_datasets) (50.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets) (3.4.0)\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:15.999078: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.206249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.207126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \u001b[0m\n",
      "\u001b[34mpciBusID: 0000:00:1e.0 name: Tesla K80 computeCapability: 3.7\u001b[0m\n",
      "\u001b[34mcoreClock: 0.8755GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.207178: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.316889: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.364629: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.377688: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.510896: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.523232: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.730508: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.730704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.731649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.732440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.734245: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.751944: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2300045000 Hz\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.752308: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4a16d30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.752345: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.979748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.980679: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4909cb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.980711: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.981652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.982451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \u001b[0m\n",
      "\u001b[34mpciBusID: 0000:00:1e.0 name: Tesla K80 computeCapability: 3.7\u001b[0m\n",
      "\u001b[34mcoreClock: 0.8755GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.982508: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.982551: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.982586: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.982614: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.982643: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.982671: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.982700: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.982781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.983642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.984425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:16.987073: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:18.095690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:18.095740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 \u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:18.095753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N \u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:18.104994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:18.105891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:18.106693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10625 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\u001b[0m\n",
      "\u001b[34m2020-11-10 17:49:18.553810: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m/opt/ml/model None\u001b[0m\n",
      "\u001b[34mGet the train data\u001b[0m\n",
      "\u001b[34mTokenize the input and output data and create the vocabularies\u001b[0m\n",
      "\u001b[34mInput vocab:  5234\u001b[0m\n",
      "\u001b[34mOutput vocab:  10042\u001b[0m\n",
      "\u001b[34mCreating the checkpoint ...\u001b[0m\n",
      "\u001b[34mTraining the model ....\u001b[0m\n",
      "\u001b[34mStarting epoch 1\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 0 Loss 3.4101 Accuracy 0.0000\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 100 Loss 3.5298 Accuracy 0.0241\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 200 Loss 3.4576 Accuracy 0.0477\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 300 Loss 3.3463 Accuracy 0.0556\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 1 in /opt/ml/checkpoints/ckpt-1\u001b[0m\n",
      "\u001b[34mStarting epoch 2\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 0 Loss 2.9338 Accuracy 0.0725\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 100 Loss 2.6994 Accuracy 0.0988\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 200 Loss 2.5204 Accuracy 0.1142\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 300 Loss 2.3763 Accuracy 0.1186\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 2 in /opt/ml/checkpoints/ckpt-2\u001b[0m\n",
      "\u001b[34mStarting epoch 3\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 0 Loss 1.9738 Accuracy 0.1272\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 100 Loss 1.8206 Accuracy 0.1388\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 200 Loss 1.7402 Accuracy 0.1495\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 300 Loss 1.6825 Accuracy 0.1544\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 3 in /opt/ml/checkpoints/ckpt-3\u001b[0m\n",
      "\u001b[34mStarting epoch 4\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 0 Loss 1.4297 Accuracy 0.1696\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 100 Loss 1.4781 Accuracy 0.1723\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 200 Loss 1.4494 Accuracy 0.1753\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 300 Loss 1.4249 Accuracy 0.1781\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 4 in /opt/ml/checkpoints/ckpt-4\u001b[0m\n",
      "\u001b[34mStarting epoch 5\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 0 Loss 1.2389 Accuracy 0.1998\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 100 Loss 1.2986 Accuracy 0.1916\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 200 Loss 1.2810 Accuracy 0.1940\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 300 Loss 1.2669 Accuracy 0.1963\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 5 in /opt/ml/checkpoints/ckpt-5\u001b[0m\n",
      "\u001b[34mSaving the model ....\u001b[0m\n",
      "\u001b[34mSaving the model parameters\u001b[0m\n",
      "\u001b[34mSaving the dictionaries ....\u001b[0m\n",
      "\u001b[34m2020-11-10 17:56:45,951 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-11-10 17:56:55 Uploading - Uploading generated training model\n",
      "2020-11-10 17:56:55 Completed - Training job completed\n",
      "Training seconds: 675\n",
      "Billable seconds: 675\n"
     ]
    }
   ],
   "source": [
    "#estimator.fit({'training':training_data_uri,'testing':testing_data_uri})\n",
    "estimator.fit({'training':training_data_uri}, job_name = job_name, \n",
    "              experiment_config = experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the experiment, then you can view it and its trials from SageMaker Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experiment(sagemaker_boto_client=<botocore.client.SageMaker object at 0x7f849be47e80>,experiment_name='tf-transformer',experiment_arn='arn:aws:sagemaker:us-east-1:223817798831:experiment/tf-transformer',display_name='tf-transformer',description='Experiment to track trainings on my tensorflow Transformer Eng-Spa',creation_time=datetime.datetime(2020, 11, 8, 17, 0, 49, 116000, tzinfo=tzlocal()),created_by={},last_modified_time=datetime.datetime(2020, 11, 9, 16, 10, 7, 126000, tzinfo=tzlocal()),last_modified_by={},response_metadata={'RequestId': 'e4e4dea1-1b3f-4e86-9571-bd0fa9a7eaf9', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'e4e4dea1-1b3f-4e86-9571-bd0fa9a7eaf9', 'content-type': 'application/x-amz-json-1.1', 'content-length': '86', 'date': 'Mon, 09 Nov 2020 16:39:02 GMT'}, 'RetryAttempts': 0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trial\n",
    "single_gpu_trial.save()\n",
    "# Save the experiment\n",
    "training_experiment.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show metrics from SageMaker Console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show where you can see the metrics from both sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach a previous training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for your the training job you want to restore the model in SageMaker console, section Training jobs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can skip the next cell if the previous estimator.fit command was executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf-transformer-single-gpu-2020-11-09-16-18-20'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the DescribeTrainingJob operation: Requested resource not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-ef90c3184c2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmy_training_job_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Attach the estimator to the selected training job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorFlow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_training_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mattach\u001b[0;34m(cls, training_job_name, sagemaker_session, model_channel_name)\u001b[0m\n\u001b[1;32m   1951\u001b[0m         \"\"\"\n\u001b[1;32m   1952\u001b[0m         estimator = super(Framework, cls).attach(\n\u001b[0;32m-> 1953\u001b[0;31m             \u001b[0mtraining_job_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_channel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1954\u001b[0m         )\n\u001b[1;32m   1955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mattach\u001b[0;34m(cls, training_job_name, sagemaker_session, model_channel_name)\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         job_details = sagemaker_session.sagemaker_client.describe_training_job(\n\u001b[0;32m--> 672\u001b[0;31m             \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_job_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         )\n\u001b[1;32m    674\u001b[0m         \u001b[0minit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_init_params_from_job_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_details\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_channel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the DescribeTrainingJob operation: Requested resource not found."
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# Set the training job you want to attach to the estimator object\n",
    "# Use this option if the training job was not trained in this execution\n",
    "#my_training_job_name = 'single-gpu-2020-11-08-18-40-33'\n",
    "\n",
    "# In case, when the training job have been trained in this execution, we can retrive the data from the job_name variable\n",
    "my_training_job_name = job_name\n",
    "# Attach the estimator to the selected training job\n",
    "estimator = TensorFlow.attach(my_training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name where the model will be restored:  tf-transformer-single-gpu-2020-11-09-16-18-20\n"
     ]
    }
   ],
   "source": [
    "print('Job name where the model will be restored: ',estimator.latest_training_job.job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir of model data:  s3://edumunozsala-ml-sagemaker/transformer-nmt/tf-transformer-single-gpu-2020-11-09-16-18-20/model.tar.gz\n",
      "Dir of output data:  s3://edumunozsala-ml-sagemaker/transformer-nmt\n",
      "Buck name:  edumunozsala-ml-sagemaker\n"
     ]
    }
   ],
   "source": [
    "print('Dir of model data: ',estimator.model_data)\n",
    "print('Dir of output data: ',output_data_uri)\n",
    "print('Buck name: ',bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir to download traned model:  transformer-nmt/tf-transformer-single-gpu-2020-11-09-16-18-20/model.tar.gz\n",
      "Dir to download model outputs:  transformer-nmt/tf-transformer-single-gpu-2020-11-09-16-18-20/output.tar.gz\n"
     ]
    }
   ],
   "source": [
    "#s3_model_path='transformer-nmt/tf-transformer-2020-11-07-17-49-03-516/output/model.tar.gz'\n",
    "init_model_path = len('s3://')+len(bucket_name)+1\n",
    "s3_model_path=estimator.model_data[init_model_path:]\n",
    "s3_output_data=output_data_uri[init_model_path:]+'/{}/output.tar.gz'.format(job_name)\n",
    "print('Dir to download traned model: ', s3_model_path)\n",
    "print('Dir to download model outputs: ', s3_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.download_data(trainedmodel_path,bucket_name,s3_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.download_data(output_data_path,bucket_name,s3_output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, extract the information out from the model.tar.gz file return by the training job in SageMaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.index\r\n",
      "checkpoint\r\n",
      "transformer.data-00000-of-00001\r\n"
     ]
    }
   ],
   "source": [
    "!tar -zxvf $trainedmodel_path/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the files from output.tar.gz without recreating the directory structure, all files will be extracted to the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/out_vocab.pkl\n",
      "data/in_vocab.pkl\n",
      "data/model_info.pth\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf $output_data_path/output.tar.gz --strip-components=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the tensorflow model and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train.model import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to restore the parameters of the model we have saved in order to build an instance of the Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters {'vocab_size_enc': 1976, 'vocab_size_dec': 3865, 'sos_token_input': [1974], 'eos_token_input': [1975], 'sos_token_output': [3863], 'eos_token_output': [3864], 'n_layers': 4, 'd_model': 64, 'ffn_dim': 128, 'n_heads': 8, 'drop_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Read the parameters from a dictionary\n",
    "#model_info_path = os.path.join(model_dir, 'model_info.pth')\n",
    "with open(model_info_file, 'rb') as f:\n",
    "    model_info = pickle.load(f)\n",
    "print('Model parameters',model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f847078cac8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an instance of the Transforer model and load the saved model to th\n",
    "transformer = Transformer(vocab_size_enc=model_info['vocab_size_enc'],\n",
    "                          vocab_size_dec=model_info['vocab_size_dec'],\n",
    "                          d_model=model_info['d_model'],\n",
    "                          n_layers=model_info['n_layers'],\n",
    "                          FFN_units=model_info['ffn_dim'],\n",
    "                          n_heads=model_info['n_heads'],\n",
    "                          dropout_rate=model_info['drop_rate'])\n",
    "\n",
    "#Load the saved model\n",
    "# Use a model_name argument to pass in on training and then apply here\n",
    "transformer.load_weights('transformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.1.0-py3-none-any.whl (3.6 MB)\n",
      "\u001b[K     || 3.6 MB 14.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (3.8.0)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (1.1.0)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     || 81 kB 15.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (0.11.0)\n",
      "Collecting typing-extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (1.18.1)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-3.3.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (1.14.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.25.0-py3-none-any.whl (44 kB)\n",
      "\u001b[K     || 44 kB 5.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=18.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (19.3.0)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.7-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (2.22.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (4.42.1)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from tensorflow-datasets) (0.18.2)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-datasets) (45.2.0.post20200210)\n",
      "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets) (2.2.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[K     || 100 kB 15.8 MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (2020.6.20)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (1.25.10)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.0.4)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21495 sha256=32366049715199ee29d105c4b94f2af4ebe0f6531f26400b8fa8e683791c5031\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "Successfully built promise\n",
      "\u001b[31mERROR: tensorflow-metadata 0.25.0 has requirement absl-py<0.11,>=0.9, but you'll have absl-py 0.11.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: dill, typing-extensions, promise, importlib-resources, googleapis-common-protos, tensorflow-metadata, dataclasses, tensorflow-datasets\n",
      "Successfully installed dataclasses-0.7 dill-0.3.3 googleapis-common-protos-1.52.0 importlib-resources-3.3.0 promise-2.3 tensorflow-datasets-4.1.0 tensorflow-metadata-0.25.0 typing-extensions-3.7.4.3\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow2_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the library necessary to tokenize the sentences\n",
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:matplotlib.font_manager:generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "from serve.predict import translate\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the input and output tokenizer or vocabularis used in the training. We need them to encode and decode the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the parameters from a dictionary\n",
    "#model_info_path = os.path.join(model_dir, 'model_info.pth')\n",
    "with open(input_vocab_file, 'rb') as f:\n",
    "    tokenizer_inputs = pickle.load(f)\n",
    "\n",
    "with open(output_vocab_file, 'rb') as f:\n",
    "    tokenizer_outputs = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: you should pay for it.\n",
      "Output sentence: \n"
     ]
    }
   ],
   "source": [
    "#Show some translations\n",
    "sentence = \"you should pay for it.\"\n",
    "print(\"Input sentence: {}\".format(sentence))\n",
    "predicted_sentence = translate(transformer,sentence,tokenizer_inputs, tokenizer_outputs,15,model_info['sos_token_input'],\n",
    "                               model_info['eos_token_input'],model_info['sos_token_output'],\n",
    "                               model_info['eos_token_output'])\n",
    "print(\"Output sentence: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: This is a really powerful method!\n",
      "Output sentence: \n"
     ]
    }
   ],
   "source": [
    "#Show some translations\n",
    "sentence = \"This is a really powerful method!\"\n",
    "print(\"Input sentence: {}\".format(sentence))\n",
    "predicted_sentence = translate(transformer,sentence,tokenizer_inputs, tokenizer_outputs,15,model_info['sos_token_input'],\n",
    "                               model_info['eos_token_input'],model_info['sos_token_output'],\n",
    "                               model_info['eos_token_output'])\n",
    "print(\"Output sentence: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the trained model to an endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy the model on sagemaker we will try to save it from the transformer model created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_fn = \"seq2seq_encoder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras model <train.model.Transformer object at 0x7f84707c7f98>, because its inputs are not defined.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: deploy_model/transformer_deploy/assets\n"
     ]
    }
   ],
   "source": [
    "deploy_model_path='deploy_model/transformer_deploy'\n",
    "tf.saved_model.save(transformer, deploy_model_path)\n",
    "#transformer.save(deploy_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = tf.saved_model.load(deploy_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `deploy()` method creates a SageMaker model, which is then deployed to an endpoint to serve prediction requests in real time. We will use the TensorFlow Serving container for the endpoint, because we trained with script mode. This serving container runs an implementation of a web server that is compatible with SageMaker hosting protocol. The [Using your own inference code]() document explains how SageMaker runs inference containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Deployed the trained TensorFlow 2.1 model to an endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker:Creating model with name: tf-transformer-2020-11-09-17-29-19-495\n",
      "INFO:sagemaker:Creating endpoint with name tf-transformer-2020-11-09-17-29-19-495\n",
      "INFO:sagemaker.local.image:serving\n",
      "INFO:sagemaker.local.image:creating hosting dir in /tmp/tmpdig2owyk\n",
      "INFO:sagemaker.local.image:docker command: docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1.0-cpu\n",
      "INFO:sagemaker.local.image:image pulled: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1.0-cpu\n",
      "INFO:sagemaker.local.image:No AWS credentials found in session but credentials from EC2 Metadata Service are available.\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-ugjn2:\n",
      "    command: serve\n",
      "    environment:\n",
      "    - SAGEMAKER_TFS_NGINX_LOGLEVEL=info\n",
      "    image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference:2.1.0-cpu\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-ugjn2\n",
      "    ports:\n",
      "    - 8080:8080\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /tmp/tmpbtgygq3x:/opt/ml/model\n",
      "version: '2.3'\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker-compose -f /tmp/tmpdig2owyk/docker-compose.yaml up --build --abort-on-container-exit\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 5\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b358>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b9b0>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9bb70>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to tmpdig2owyk_algo-1-ugjn2_1\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m INFO:__main__:starting services\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m   File \"/sagemaker/serve.py\", line 388, in <module>\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m     ServiceManager().start()\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m   File \"/sagemaker/serve.py\", line 342, in start\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m     self._create_tfs_config()\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m   File \"/sagemaker/serve.py\", line 92, in _create_tfs_config\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m     raise ValueError('no SavedModel bundles found!')\n",
      "\u001b[36malgo-1-ugjn2_1  |\u001b[0m ValueError: no SavedModel bundles found!\n",
      "\u001b[36mtmpdig2owyk_algo-1-ugjn2_1 exited with code 1\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 627, in run\n",
      "    _stream_output(self.process)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 687, in _stream_output\n",
      "    raise RuntimeError(\"Process exited with code: %s\" % exit_code)\n",
      "RuntimeError: Process exited with code: 1\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\", line 632, in run\n",
      "    raise RuntimeError(msg)\n",
      "RuntimeError: Failed to run: ['docker-compose', '-f', '/tmp/tmpdig2owyk/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1\n",
      "\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 10\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9ba58>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca79198>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca792b0>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 15\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845c9b0ba8>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845c9b0be0>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b438>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 20\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b940>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b358>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9bf60>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 25\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b2b0>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b6d8>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f849b2a2240>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 30\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845c9bfa90>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca79390>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca794a8>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 35\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca796a0>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca79860>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845c9bfac8>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 40\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b518>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9beb8>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9ba20>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 45\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca9b320>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845c9b0da0>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845c9b0f98>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 50\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845c9b0c88>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca32400>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f849b2a2470>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 55\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca797b8>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca79978>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca794e0>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n",
      "INFO:sagemaker.local.entities:Checking if serving container is up, attempt: 60\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca79a90>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca79c50>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f845ca79d68>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "INFO:sagemaker.local.entities:Container still not up, got: -1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-0d1b68be025b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         )\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/tensorflow/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, update_endpoint)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mupdate_endpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate_endpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         )\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\u001b[0m\n\u001b[1;32m   3191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   2706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m         self.sagemaker_client.create_endpoint(\n\u001b[0;32m-> 2708\u001b[0;31m             \u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m         )\n\u001b[1;32m   2710\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, EndpointName, EndpointConfigName, Tags)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mendpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LocalEndpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mserve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0mserving_port\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local.serving_port\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m8080\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         \u001b[0m_wait_for_serving_container\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserving_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m         \u001b[0;31m# the container is running and it passed the healthcheck status is now InService\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LocalEndpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_IN_SERVICE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36m_wait_for_serving_container\u001b[0;34m(serving_port)\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume training from a checkpoint\n",
    "\n",
    "Let's download the training data and use that as input for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name='tf-transformer'\n",
    "trial_name='single-gpu'\n",
    "trial_comp_name = 'single-gpu-training-job'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the experiment\n",
      "Load the trial\n"
     ]
    }
   ],
   "source": [
    "# create the experiment if it doesn't exist\n",
    "try:\n",
    "    experiment = Experiment.load(experiment_name=experiment_name)\n",
    "    print('Load the experiment')\n",
    "except Exception as ex:\n",
    "    if \"ResourceNotFound\" in str(ex):\n",
    "        experiment = Experiment.create(experiment_name=experiment_name)\n",
    "        print('Create the experiment')\n",
    "\n",
    "\n",
    "# create the trial if it doesn't exist\n",
    "try:\n",
    "    trial = Trial.load(trial_name=trial_name)\n",
    "    print('Load the trial')\n",
    "except Exception as ex:\n",
    "    if \"ResourceNotFound\" in str(ex):\n",
    "        trial = Trial.create(experiment_name=experiment_name, trial_name=trial_name)\n",
    "        print('Create the trial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the configuration parameters for the experiment\n",
    "experiment_config = {'ExperimentName': experiment.experiment_name, \n",
    "                       'TrialName': trial.trial_name,\n",
    "                       'TrialComponentDisplayName': trial_comp_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Estimator for a TensorFlow 2.1 model and set the parameter `--resume` to True to force the model to restore the latest checkpoint and resume training for the number of epochs selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instance_type='ml.m5.xlarge'\n",
    "#instance_type='ml.m4.4xlarge'\n",
    "instance_type='ml.p2.xlarge'\n",
    "#instance_type='local'\n",
    "\n",
    "# Define the metrics to search for\n",
    "metric_definitions = [{'Name': 'loss', 'Regex': 'Loss ([0-9\\\\.]+)'},{'Name': 'Accuracy', 'Regex': 'Accuracy ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir=\"train\",\n",
    "                       role=role,\n",
    "                       instance_count=1,\n",
    "                       instance_type=instance_type,\n",
    "                       framework_version='2.1.0',\n",
    "                       py_version='py3',\n",
    "                       output_path=output_data_uri,\n",
    "                       code_location=output_data_uri,\n",
    "                       base_job_name='tf-transformer',\n",
    "                       script_mode= True,\n",
    "                       checkpoint_s3_uri = ckpt_data_uri,\n",
    "                       metric_definitions = metric_definitions, \n",
    "                       hyperparameters={\n",
    "                        'epochs': 5,\n",
    "                        'nsamples': 40000,\n",
    "                        'resume': True,\n",
    "                        'train_file': 'spa.txt',\n",
    "                        'non_breaking_in': 'nonbreaking_prefix.en',\n",
    "                        'non_breaking_out': 'nonbreaking_prefix.es'\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single-gpu-2020-11-08-18-40-33\n"
     ]
    }
   ],
   "source": [
    "#job_name=f'tensorflow-single-gpu-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())}'\n",
    "job_name = '{}-{}'.format(trial_name,time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime()))\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: single-gpu-2020-11-08-18-40-33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-08 18:40:47 Starting - Starting the training job...\n",
      "2020-11-08 18:41:16 Starting - Launching requested ML instances.........\n",
      "2020-11-08 18:42:47 Starting - Preparing the instances for training.........\n",
      "2020-11-08 18:44:02 Downloading - Downloading input data...\n",
      "2020-11-08 18:44:28 Training - Downloading the training image........\u001b[34m2020-11-08 18:45:59,732 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-11-08 18:46:00,300 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"resume\": true,\n",
      "        \"non_breaking_out\": \"nonbreaking_prefix.es\",\n",
      "        \"nsamples\": 40000,\n",
      "        \"train_file\": \"spa.txt\",\n",
      "        \"model_dir\": \"s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/model\",\n",
      "        \"non_breaking_in\": \"nonbreaking_prefix.en\",\n",
      "        \"epochs\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"single-gpu-2020-11-08-18-40-33\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":5,\"model_dir\":\"s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/model\",\"non_breaking_in\":\"nonbreaking_prefix.en\",\"non_breaking_out\":\"nonbreaking_prefix.es\",\"nsamples\":40000,\"resume\":true,\"train_file\":\"spa.txt\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":5,\"model_dir\":\"s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/model\",\"non_breaking_in\":\"nonbreaking_prefix.en\",\"non_breaking_out\":\"nonbreaking_prefix.es\",\"nsamples\":40000,\"resume\":true,\"train_file\":\"spa.txt\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"single-gpu-2020-11-08-18-40-33\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"5\",\"--model_dir\",\"s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/model\",\"--non_breaking_in\",\"nonbreaking_prefix.en\",\"--non_breaking_out\",\"nonbreaking_prefix.es\",\"--nsamples\",\"40000\",\"--resume\",\"True\",\"--train_file\",\"spa.txt\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_RESUME=true\u001b[0m\n",
      "\u001b[34mSM_HP_NON_BREAKING_OUT=nonbreaking_prefix.es\u001b[0m\n",
      "\u001b[34mSM_HP_NSAMPLES=40000\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=spa.txt\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/model\u001b[0m\n",
      "\u001b[34mSM_HP_NON_BREAKING_IN=nonbreaking_prefix.en\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 train.py --epochs 5 --model_dir s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/model --non_breaking_in nonbreaking_prefix.en --non_breaking_out nonbreaking_prefix.es --nsamples 40000 --resume True --train_file spa.txt\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mCollecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.1.0-py3-none-any.whl (3.6 MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (3.11.3)\u001b[0m\n",
      "\u001b[34mCollecting dill\n",
      "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.1.0)\u001b[0m\n",
      "\u001b[34mCollecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.7-py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-3.3.0-py2.py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-0.25.0-py3-none-any.whl (44 kB)\u001b[0m\n",
      "\u001b[34mCollecting attrs>=18.1.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (0.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.18.1)\u001b[0m\n",
      "\u001b[34mCollecting tqdm\n",
      "  Downloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\u001b[0m\n",
      "\u001b[34mCollecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting typing-extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow_datasets) (46.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.52.0-py2.py3-none-any.whl (100 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.25.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2020.4.5.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: promise, future\n",
      "  Building wheel for promise (setup.py): started\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-11-08 18:45:54 Training - Training image download completed. Training in progress.\u001b[34m  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21495 sha256=0e4a1bcae031239e48de28b80da81a42374fe6a78c4f4fe531599aada4269554\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=ddac4fdd0607303ad63138a91b4c623df4d1244fbac32d172ab477dfaedf8e65\n",
      "  Stored in directory: /root/.cache/pip/wheels/6e/9c/ed/4499c9865ac1002697793e0ae05ba6be33553d098f3347fb94\u001b[0m\n",
      "\u001b[34mSuccessfully built promise future\u001b[0m\n",
      "\u001b[34mInstalling collected packages: dill, dataclasses, importlib-resources, googleapis-common-protos, tensorflow-metadata, attrs, tqdm, promise, typing-extensions, future, tensorflow-datasets\u001b[0m\n",
      "\u001b[34mSuccessfully installed attrs-20.3.0 dataclasses-0.7 dill-0.3.3 future-0.18.2 googleapis-common-protos-1.52.0 importlib-resources-3.3.0 promise-2.3 tensorflow-datasets-4.1.0 tensorflow-metadata-0.25.0 tqdm-4.51.0 typing-extensions-3.7.4.3\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m/opt/ml/model s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/model\u001b[0m\n",
      "\u001b[34mGet the train data\u001b[0m\n",
      "\u001b[34mTokenize the input and output data and create the vocabularies\u001b[0m\n",
      "\u001b[34mInput vocab:  8596\u001b[0m\n",
      "\u001b[34mOutput vocab:  9417\u001b[0m\n",
      "\u001b[34mCreating the checkpoint ...\u001b[0m\n",
      "\u001b[34mLast checkpoint restored.\u001b[0m\n",
      "\u001b[34mTraining the model ....\u001b[0m\n",
      "\u001b[34mStarting epoch 1\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 0 Loss 1.3094 Accuracy 0.2690\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 100 Loss 1.1614 Accuracy 0.2596\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 200 Loss 1.1689 Accuracy 0.2606\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 300 Loss 1.1686 Accuracy 0.2608\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 400 Loss 1.1670 Accuracy 0.2621\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 500 Loss 1.1663 Accuracy 0.2636\u001b[0m\n",
      "\u001b[34mEpoch 1 Batch 600 Loss 1.1635 Accuracy 0.2645\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 1 in /opt/ml/checkpoints/ckpt-6\u001b[0m\n",
      "\u001b[34mStarting epoch 2\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 0 Loss 1.1658 Accuracy 0.2723\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 100 Loss 1.0105 Accuracy 0.2810\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 200 Loss 1.0265 Accuracy 0.2800\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 300 Loss 1.0315 Accuracy 0.2805\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 400 Loss 1.0334 Accuracy 0.2809\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 500 Loss 1.0364 Accuracy 0.2813\u001b[0m\n",
      "\u001b[34mEpoch 2 Batch 600 Loss 1.0383 Accuracy 0.2818\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 2 in /opt/ml/checkpoints/ckpt-7\u001b[0m\n",
      "\u001b[34mStarting epoch 3\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 0 Loss 0.9556 Accuracy 0.3013\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 100 Loss 0.8900 Accuracy 0.2980\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 200 Loss 0.9018 Accuracy 0.2975\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 300 Loss 0.9108 Accuracy 0.2974\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 400 Loss 0.9162 Accuracy 0.2974\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 500 Loss 0.9190 Accuracy 0.2972\u001b[0m\n",
      "\u001b[34mEpoch 3 Batch 600 Loss 0.9198 Accuracy 0.2976\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 3 in /opt/ml/checkpoints/ckpt-8\u001b[0m\n",
      "\u001b[34mStarting epoch 4\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 0 Loss 0.8783 Accuracy 0.3259\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 100 Loss 0.7916 Accuracy 0.3111\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 200 Loss 0.8035 Accuracy 0.3100\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 300 Loss 0.8117 Accuracy 0.3101\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 400 Loss 0.8199 Accuracy 0.3103\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 500 Loss 0.8245 Accuracy 0.3101\u001b[0m\n",
      "\u001b[34mEpoch 4 Batch 600 Loss 0.8283 Accuracy 0.3097\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 4 in /opt/ml/checkpoints/ckpt-9\u001b[0m\n",
      "\u001b[34mStarting epoch 5\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 0 Loss 0.7558 Accuracy 0.3382\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 100 Loss 0.7098 Accuracy 0.3224\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 200 Loss 0.7211 Accuracy 0.3213\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 300 Loss 0.7341 Accuracy 0.3210\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 400 Loss 0.7460 Accuracy 0.3201\u001b[0m\n",
      "\u001b[34mEpoch 5 Batch 500 Loss 0.7556 Accuracy 0.3191\u001b[0m\n",
      "\n",
      "2020-11-08 19:04:29 Uploading - Uploading generated training model\u001b[34mEpoch 5 Batch 600 Loss 0.7605 Accuracy 0.3186\u001b[0m\n",
      "\u001b[34mSaving checkpoint for epoch 5 in /opt/ml/checkpoints/ckpt-10\u001b[0m\n",
      "\u001b[34mSaving the model ....\u001b[0m\n",
      "\u001b[34mSaving the model parameters\u001b[0m\n",
      "\u001b[34mSaving the dictionaries ....\u001b[0m\n",
      "\u001b[34m2020-11-08 19:04:28,823 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving container. The model artifact was not saved in the TensorFlow SavedModel directory structure:\u001b[0m\n",
      "\u001b[34mhttps://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\u001b[0m\n",
      "\u001b[34m2020-11-08 19:04:28,823 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-11-08 19:04:37 Completed - Training job completed\n",
      "Training seconds: 1235\n",
      "Billable seconds: 1235\n"
     ]
    }
   ],
   "source": [
    "# Fit or train the model from the latest checkpoint\n",
    "estimator.fit({'training':training_data_uri}, job_name = job_name, \n",
    "              experiment_config = experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.00026031278, 0.990563631, 0.00917605218],\n",
       " [0.999760091, 0.000239968373, 3.97464422e-10],\n",
       " [0.000185193509, 0.974752605, 0.0250621513],\n",
       " [9.90088935e-08, 0.241644651, 0.758355319],\n",
       " [1.86230598e-09, 0.0252015758, 0.974798381]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_experiment.delete_all(action=\"--force\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete the endpoint\n",
    "\n",
    "Let's delete the endpoint we just created to prevent incurring any extra costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the TensorFlow 2.1 endpoint as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n"
     ]
    }
   ],
   "source": [
    "estimator.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy model using artifacts\n",
    "https://sagemaker.readthedocs.io/en/stable/using_tf.html#deploy-to-a-sagemaker-endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = estimator.model_data\n",
    "instance_type='ml.m4.4xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://edumunozsala-ml-sagemaker/transformer-nmt/single-gpu-2020-11-08-18-40-33/output/model.tar.gz'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.deprecations:The class sagemaker.tensorflow.serving.Model has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "WARNING:sagemaker.deprecations:update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "INFO:sagemaker:Creating model with name: tensorflow-inference-2020-11-08-19-26-16-925\n",
      "INFO:sagemaker:Creating endpoint with name tensorflow-inference-2020-11-08-19-26-17-266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint tensorflow-inference-2020-11-08-19-26-17-266: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint..",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-8caaeacd937d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframework_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2.1.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/tensorflow/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, update_endpoint)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mupdate_endpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate_endpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mkms_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkms_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_capture_config_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m         )\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\u001b[0m\n\u001b[1;32m   3191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3193\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   2709\u001b[0m         )\n\u001b[1;32m   2710\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2711\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2712\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_endpoint\u001b[0;34m(self, endpoint, poll)\u001b[0m\n\u001b[1;32m   2978\u001b[0m                 ),\n\u001b[1;32m   2979\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"InService\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2980\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2981\u001b[0m             )\n\u001b[1;32m   2982\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint tensorflow-inference-2020-11-08-19-26-17-266: Failed. Reason:  The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.."
     ]
    }
   ],
   "source": [
    "model = Model(model_data=model_data, role=role,framework_version='2.1.0')\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.8, 2.7, 4.1, 1. ],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-oufpd_1  |\u001b[0m 172.18.0.1 - - [28/Mar/2020:21:15:01 +0000] \"POST /invocations HTTP/1.1\" 200 254 \"-\" \"-\"\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.00026031278, 0.990563631, 0.00917605218],\n",
       "  [0.999760091, 0.000239968373, 3.97464422e-10],\n",
       "  [0.000185193509, 0.974752605, 0.0250621513],\n",
       "  [9.90088935e-08, 0.241644651, 0.758355319],\n",
       "  [1.86230598e-09, 0.0252015758, 0.974798381]]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Referencias for experiment and trial\n",
    "https://github.com/shashankprasanna/sagemaker-training-tutorial/blob/master/sagemaker-training-tutorial.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
